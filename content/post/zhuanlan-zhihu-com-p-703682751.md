---
title: '量子位发表了文章: 37项SOTA！全模态预训练范式MiCo：理解任何模态并学习通用表示'
date: '2024-06-16'
linkTitle: https://zhuanlan.zhihu.com/p/703682751
source: 量子位的知乎动态
description: <blockquote data-pid="IPEuf-8e">MiCo团队 投稿<br>量子位 | 公众号 QbitAI</blockquote><p
  data-pid="kBMgGnTB">GPT-4o掀起一股<b>全模态</b>（Omni-modal）热潮，去年的热词<b>多模态</b>仿佛已经不够看了。</p><p
  data-pid="nqD2YSlj">要构建全模态智能，意味着能够理解任何模态并学习通用表示 (Universal Representations)。</p><p
  data-pid="XRwO0Bw3">现在，港中文、中科院等提出了一种大规模的全模态预训练范式，称为<b>多模态上下文MiCo</b>（Multimodal
  Context），它可以在预训练过程中引入更多的模态，数据量，模型参数。</p><p class="ztext-empty-paragraph"><br></p><figure
  data-size="normal"><img src="https://pic2.zhimg.com/v2-c187db956 ...
disable_comments: true
---
<blockquote data-pid="IPEuf-8e">MiCo团队 投稿<br>量子位 | 公众号 QbitAI</blockquote><p data-pid="kBMgGnTB">GPT-4o掀起一股<b>全模态</b>（Omni-modal）热潮，去年的热词<b>多模态</b>仿佛已经不够看了。</p><p data-pid="nqD2YSlj">要构建全模态智能，意味着能够理解任何模态并学习通用表示 (Universal Representations)。</p><p data-pid="XRwO0Bw3">现在，港中文、中科院等提出了一种大规模的全模态预训练范式，称为<b>多模态上下文MiCo</b>（Multimodal Context），它可以在预训练过程中引入更多的模态，数据量，模型参数。</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><img src="https://pic2.zhimg.com/v2-c187db956 ...